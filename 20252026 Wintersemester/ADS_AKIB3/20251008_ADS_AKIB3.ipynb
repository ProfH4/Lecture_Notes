{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4944b0f2-ae46-4ba7-aa5a-7aec4d5dc607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV Beispiel-CSV aus String\n",
      "[INFO] Rohform:\n",
      "                     date     city  rooms  area_m2     price has_elevator\n",
      "0              2025-01-03   Berlin    2.0     45.0  245000.0          yes\n",
      "1              2025-02-10   berlin    NaN     60.0  315000.0           no\n",
      "2              2024-12-15  München    1.0     34.0       NaN          yes\n",
      "3              2025-01-20  Hamburg    3.0     85.0  520000.0          yes\n",
      "4              2025-03-02  München    2.0      NaN  480000.0           no \n",
      "\n",
      "[INFO] Duplikate entfernt: 0\n",
      "\n",
      "[INFO] Spalten-Typen:\n",
      "  numerisch: ['rooms', 'area_m2', 'date_year', 'date_month', 'date_dow']\n",
      "  kategorisch: ['city', 'has_elevator']\n",
      "  datetime: ['date'] \n",
      "\n",
      "[INFO] Nach Vorbereitung:\n",
      "  X_train_pre Form: (4, 11)\n",
      "  X_test_pre  Form: (2, 11)\n",
      "  Beispielspalten: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9'] \n",
      "\n",
      "[CHECK] Fehlende Werte nach Pipeline (Train): 0\n",
      "\n",
      "[STAT] Beschreibende Statistik der vorbereiteten Features (Train):\n",
      "     count          mean       std       min           25%       50%  \\\n",
      "f0     4.0 -1.110223e-16  1.154701 -1.414214 -3.535534e-01  0.000000   \n",
      "f1     4.0  1.110223e-16  1.154701 -1.279507 -4.127442e-01 -0.123823   \n",
      "f2     4.0  1.353917e-13  1.154701 -1.732051  1.353362e-13  0.577350   \n",
      "f3     4.0  8.326673e-17  1.154701 -0.879316 -5.275893e-01 -0.410347   \n",
      "f4     4.0 -5.551115e-17  1.154701 -1.340227 -7.758191e-01  0.188136   \n",
      "f5     4.0  2.500000e-01  0.500000  0.000000  0.000000e+00  0.000000   \n",
      "f6     4.0  2.500000e-01  0.500000  0.000000  0.000000e+00  0.000000   \n",
      "f7     4.0  5.000000e-01  0.577350  0.000000  0.000000e+00  0.500000   \n",
      "f8     4.0  2.500000e-01  0.500000  0.000000  0.000000e+00  0.000000   \n",
      "f9     4.0  2.500000e-01  0.500000  0.000000  0.000000e+00  0.000000   \n",
      "f10    4.0  5.000000e-01  0.577350  0.000000  0.000000e+00  0.500000   \n",
      "\n",
      "          75%       max  \n",
      "f0   0.353553  1.414214  \n",
      "f1   0.288921  1.527153  \n",
      "f2   0.577350  0.577350  \n",
      "f3   0.117242  1.700010  \n",
      "f4   0.963955  0.963955  \n",
      "f5   0.250000  1.000000  \n",
      "f6   0.250000  1.000000  \n",
      "f7   1.000000  1.000000  \n",
      "f8   0.250000  1.000000  \n",
      "f9   0.250000  1.000000  \n",
      "f10  1.000000  1.000000   \n",
      "\n",
      "[INFO] 'prepared_train.csv' und 'prepared_test.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vorlesung 2 – Datenvorbereitung: EIN kommentiertes Snippet\n",
    "Themen (aus Teil A): CSV laden, Typen, Missingness, Imputation, Indikatoren, One-Hot, Skalierung\n",
    "(Standardisierung/MinMax/Robust), Ausreißer-Kappen, Datumsfeatures, Duplikate, Train/Test, Pipeline.\n",
    "\n",
    "Das Skript versucht zuerst, 'data.csv' aus dem Arbeitsverzeichnis zu laden.\n",
    "Falls nicht vorhanden, wird ein kleines Beispiel-CSV aus einem String erzeugt.\n",
    "\"\"\"\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) CSV LADEN (A8): robust mit na_values, Encoding, Fallback\n",
    "# ------------------------------------------------------------\n",
    "CSV_PATH = \"data.csv\"\n",
    "\n",
    "NA_TOKENS = [\"\", \" \", \"NA\", \"N/A\", \"n/a\", \"NaN\", \"nan\", \"NULL\", \"null\", \"?\", \"-\", \"--\"]\n",
    "\n",
    "def load_csv_or_example(path):\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            sep=\",\",               # passe an, falls ';'\n",
    "            decimal=\".\",           # passe an, falls ','\n",
    "            encoding=\"utf-8\",\n",
    "            na_values=NA_TOKENS,\n",
    "            low_memory=False\n",
    "        )\n",
    "        source = f\"geladen: {path}\"\n",
    "    except FileNotFoundError:\n",
    "        demo = io.StringIO(\n",
    "            \"\"\"date,city,rooms,area_m2,price,has_elevator\n",
    "            2025-01-03,Berlin,2,45,245000,yes\n",
    "            2025-02-10,berlin,NA,60,315000,no\n",
    "            2024-12-15,München,1,34,?,yes\n",
    "            2025-01-20,Hamburg,3,85,520000,yes\n",
    "            2025-03-02,München,2, ,480000,no\n",
    "            2025-03-05,Berlin,2,55,310000,unknown\n",
    "            \"\"\"\n",
    "        )\n",
    "        df = pd.read_csv(demo, sep=\",\", na_values=NA_TOKENS)\n",
    "        source = \"Beispiel-CSV aus String\"\n",
    "    return df, source\n",
    "\n",
    "df, src = load_csv_or_example(CSV_PATH)\n",
    "print(f\"[INFO] CSV {src}\")\n",
    "print(\"[INFO] Rohform:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) SPALTENNAMEN & GRUNDBEREINIGUNG (A9)\n",
    "# ------------------------------------------------------------\n",
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.strip()\n",
    "      .str.lower()\n",
    "      .str.replace(\" \", \"_\", regex=False)\n",
    ")\n",
    "\n",
    "# Strings trimmen, leere Strings -> NaN\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    df.loc[df[col].isin([\"\", \"nan\", \"None\", \"none\", \"NaN\"]), col] = np.nan\n",
    "\n",
    "# Duplikate entfernen\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "print(f\"[INFO] Duplikate entfernt: {before - len(df)}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) DATUMS-/ZEITSPALTEN PARSEN (A7)\n",
    "# ------------------------------------------------------------\n",
    "for col in df.columns:\n",
    "    if any(tok in col for tok in [\"date\", \"time\", \"datum\"]):\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "date_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.datetime64)]\n",
    "for col in date_cols:\n",
    "    df[col + \"_year\"]  = df[col].dt.year\n",
    "    df[col + \"_month\"] = df[col].dt.month\n",
    "    df[col + \"_dow\"]   = df[col].dt.dayofweek\n",
    "# df = df.drop(columns=date_cols)  # optional\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) TYPEN & KATEGORIEN (A1, A9)\n",
    "# ------------------------------------------------------------\n",
    "cat_cols_guess = list(df.select_dtypes(include=[\"object\"]).columns)\n",
    "for col in cat_cols_guess:\n",
    "    df[col] = df[col].str.lower()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) ZIEL (optional) & FEATURE-SELEKTION\n",
    "# ------------------------------------------------------------\n",
    "TARGET_COL = \"price\" if \"price\" in df.columns else None\n",
    "\n",
    "if TARGET_COL is not None:\n",
    "    y = df[TARGET_COL].astype(float)  # erzwinge numerisch\n",
    "    X = df.drop(columns=[TARGET_COL])\n",
    "else:\n",
    "    y = None\n",
    "    X = df.copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) SPALTENTYPEN ERKENNEN\n",
    "# ------------------------------------------------------------\n",
    "num_cols = list(X.select_dtypes(include=[np.number]).columns)\n",
    "cat_cols = list(X.select_dtypes(include=[\"object\", \"category\"]).columns)\n",
    "dt_cols  = [c for c in X.columns if np.issubdtype(X[c].dtype, np.datetime64)]\n",
    "\n",
    "print(\"[INFO] Spalten-Typen:\")\n",
    "print(\"  numerisch:\", num_cols)\n",
    "print(\"  kategorisch:\", cat_cols)\n",
    "print(\"  datetime:\", dt_cols, \"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) TRAIN/TEST SPLIT (A10)\n",
    "# ------------------------------------------------------------\n",
    "if y is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "else:\n",
    "    X_train, X_test = train_test_split(X, test_size=0.25, random_state=42)\n",
    "    y_train = y_test = None\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) BENUTZERWAHL: SKALIERUNG (A4)\n",
    "# ------------------------------------------------------------\n",
    "SCALE_KIND = \"zscore\"  # 'zscore' | 'minmax' | 'robust'\n",
    "\n",
    "if SCALE_KIND == \"zscore\":\n",
    "    scaler = StandardScaler()\n",
    "elif SCALE_KIND == \"minmax\":\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "elif SCALE_KIND == \"robust\":\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n",
    "else:\n",
    "    raise ValueError(\"SCALE_KIND muss 'zscore', 'minmax' oder 'robust' sein.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) AUSREIẞER-KAPPEN (A5) (Windosirized)\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class QuantileClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, q_low=0.01, q_high=0.99):\n",
    "        self.q_low = q_low\n",
    "        self.q_high = q_high\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.lower_ = X.quantile(self.q_low, axis=0)\n",
    "        self.upper_ = X.quantile(self.q_high, axis=0)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, [\"lower_\", \"upper_\"])\n",
    "        X = pd.DataFrame(X)\n",
    "        X = X.clip(lower=self.lower_.values, upper=self.upper_.values, axis=1)\n",
    "        return X.values\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9) PIPELINES (A2, A4, A6, A10)\n",
    "# ------------------------------------------------------------\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"clip\", QuantileClipper(q_low=0.01, q_high=0.99)),\n",
    "    (\"scaler\", scaler),\n",
    "])\n",
    "\n",
    "# --- OneHotEncoder: kompatibel zu sklearn>=1.2 (sparse_output) und älter (sparse) ---\n",
    "def make_ohe():\n",
    "    try:\n",
    "        # Neuere sklearn-Versionen\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        # Ältere sklearn-Versionen\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\", sparse=False)\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", make_ohe()),\n",
    "])\n",
    "\n",
    "# Datumswerte droppen (Roh-datetime), abgeleitete Features behalten wir schon oben\n",
    "drop_cols = dt_cols\n",
    "X_train_ = X_train.drop(columns=drop_cols) if drop_cols else X_train\n",
    "X_test_  = X_test.drop(columns=drop_cols)  if drop_cols else X_test\n",
    "\n",
    "# Spaltenlisten nach Drop aktualisieren\n",
    "num_cols = [c for c in num_cols if c in X_train_.columns]\n",
    "cat_cols = [c for c in cat_cols if c in X_train_.columns]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=True,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10) FIT/TRANSFORM\n",
    "# ------------------------------------------------------------\n",
    "if y_train is not None:\n",
    "    _ = preprocess.fit(X_train_, y_train)\n",
    "else:\n",
    "    _ = preprocess.fit(X_train_)\n",
    "\n",
    "X_train_pre = preprocess.transform(X_train_)\n",
    "X_test_pre  = preprocess.transform(X_test_)\n",
    "\n",
    "# Feature-Namen\n",
    "try:\n",
    "    feat_names = preprocess.get_feature_names_out()\n",
    "except Exception:\n",
    "    feat_names = [f\"f{i}\" for i in range(X_train_pre.shape[1])]\n",
    "\n",
    "X_train_pre_df = pd.DataFrame(X_train_pre, columns=feat_names, index=X_train_.index)\n",
    "X_test_pre_df  = pd.DataFrame(X_test_pre,  columns=feat_names, index=X_test_.index)\n",
    "\n",
    "print(\"[INFO] Nach Vorbereitung:\")\n",
    "print(\"  X_train_pre Form:\", X_train_pre_df.shape)\n",
    "print(\"  X_test_pre  Form:\", X_test_pre_df.shape)\n",
    "print(\"  Beispielspalten:\", list(X_train_pre_df.columns[:min(10, X_train_pre_df.shape[1])]), \"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 11) DIAGNOSTIK\n",
    "# ------------------------------------------------------------\n",
    "missing_after = X_train_pre_df.isna().sum().sum()\n",
    "print(f\"[CHECK] Fehlende Werte nach Pipeline (Train): {missing_after}\")\n",
    "\n",
    "desc = X_train_pre_df.describe().T\n",
    "print(\"\\n[STAT] Beschreibende Statistik der vorbereiteten Features (Train):\")\n",
    "print(desc.head(12), \"\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 12) EXPORT\n",
    "# ------------------------------------------------------------\n",
    "X_train_pre_df.to_csv(\"prepared_train.csv\", index=False)\n",
    "X_test_pre_df.to_csv(\"prepared_test.csv\", index=False)\n",
    "print(\"[INFO] 'prepared_train.csv' und 'prepared_test.csv' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319d322-e6b0-46ef-86ae-5c97b085de25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
